---
title: |
  | STATS 3DS3
  | Homework assignment 3
author: 
  - "Ling Lu"
date: "02/20/2021"
output: 
  pdf_document:
    includes:
      in_header: header.tex
bibliography: STATS3DS3.bib

---

**Submit to Crowdmark using the link that was emailed to you.**

**Due before 10:00 PM on Monday, March 1st, 2021.**

**Assignments submitted after the due date will receive a zero grade.**

**Answer all the questions.** 

**Grading scheme: $\left\lbrace 0, 1, 2\right\rbrace$ points per question, including reference section, total of 34. We will convert this to 100\%.**

**Your assignment must conform to the Assignment Standards listed below.**

* Write your name and student number on the title page. Submit the title page for Q0. There is no grade for Q0. We will not grade assignments without Q0. 

* You may discuss homework problems with other students, but you have to prepare the written assignments yourself. You can mention one helper's name in the helper's section only. 

* Please combine all your answers, the computer code, and the figures into one PDF file, and submit a copy to Crowdmark.

* Please use **newpage** to write a solution for each part of a question.

* Please choose the pages for each part of a question in Crowdmark. 

* No screenshots are accepted for any reason.

* The writing and referencing should be appropriate to the undergraduate level.

* Various tools, including publicly available internet tools, may be used by the instructor to check the originality of submitted work.



\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      collapse = TRUE, 
                      comment = "#>")

```

## Question 1

1.1 Consider the diagram below. It shows labeled observations from two classes, class "blue circle" and class "red square". It also shows an unlabelled observation depicted by the green triangle. Using K-Nearest Neighbours classification, which class is the unlabelled observation assigned to when;

$K = 1$

$K = 3$

$K = 6$



```{r include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggforce)
library(MASS)
```


```{r echo=FALSE, fig.align='center'}
ggplot() +
  geom_circle(data = tibble(x = c(2), y= c(3), r = c(1.5)), aes(x0 = x, y0 = y, r = r), linetype = "dashed") +
  geom_circle(data = tibble(x = c(2), y= c(3), r = c(3)), aes(x0 = x, y0 = y, r = r)) +
   coord_fixed() +
  xlab(expression(x[1])) +
  ylab(expression(x[2])) +
  geom_point(data = tibble(x1 = seq(-0.5, 5, length.out = 12 ), x2 = c(0.1, 2, 1.5, 4, 2.5, 3, 1.3, 2.5, 5.8, 5.8, 1, 4.2), class = c(rep("blue", 5), "green", rep("red", 6))), aes(x = x1, y = x2, color = class, fill = class, shape = class), size = 4) + theme_bw() + 
  scale_color_manual(values = c("blue", "green", "red"))
```
when K=1, the blue circle is the unlabelled observation.
k=3, the blue circle is the unlabelled observation
k=6, the blue circle is the unlabelled observation
\newpage 
1.2 For this question, we will use _iris_ data in _datasets_ package. The help page says that _iris_ is a famous (Fisher's or Anderson's) _iris_ data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of Iris. The species are Iris _setosa_, _versicolor_, and _virginica_. Use `summary()` to explore the variables in the _iris_ data. Then, comment on the results. 
```{r}
library(datasets)
data(iris)
summary(iris)

```
 there are 3 species of Iris in the dataset. the smallest sepal length of all 3 species is 4.3 and the largest sepal length is 7.9 with median of 5.8. The sepal width of Iris is between 2.0 to 4.4 with median of 3.0. The minimum petal length of Iris is 1.0 while the maximum is 6.9. Last but not least, the petal width of Iris is between 0.1 and 2.5. the mean for sepal.length, sepal.width, petal.length and petal width are 5.843,3.057,3.758and 1.199 respectively.

\newpage

1.3 Identify quantitative and qualitative variables in _iris_ data. 

the qualitative variable is species.
the quantitative varianles is sepal length, sepal width, petal lenth and petal width.

\newpage
1.4 Suppose we want to predict species type of Iris plant using sepal length, sepal width, petal length, petal width. Identify 
the type of statistical learning method that we can use for this task. 

Supervised learning method(classification)

\newpage
1.5 Use `cor()` function to produce a matrix that contains all of the pairwise correlations among the predictors in the iris data set. Comment on the pairwise correlations. 

```{r}
library(datasets)
data("iris")
attach(iris)
iris=data.frame(Sepal.Length,Sepal.Width,Petal.Length,Petal.Width)
cor(iris)
```
the correlations between themselves are all the same which is 1. sepal.Length has strong positive corrlation with petal.length and peral.width while it has weak negative correlation with sepal.Width since its only 0.1176. sepal.width has weak correlation with all other variables. Petal.length has strong correlation with all other variables except sepal.width.

\newpage

1.6 Create a pairwise plot matrix for all response and all predictor variables in the _iris_ data set and comment on the plot.

```{r}
pairs(iris[,1:4])
```

sepal.length and sepal width seem to have a weak correlation while sepal.length has strong postive correlation with petal.length and petal.width. the correlation between sepal.width and petal.length seem to be weak positive(same for sepal.width and petal.width). petal.length and petal.width have strong postive correlation.


\newpage

1.7 This question asks to comment on a confusion matrix. We will look at the steps of fitting KNN. We use stratified sampling to select observations for the training data. Let's say we choose 50\%/50\% (training and hold-out). 

We can find out the proportion of each species type in the whole data set.

```{r}
library(datasets)
data("iris")
set.seed(1)
iris %>% 
  dplyr::group_by(Species) %>%
  summarise(n = n()) %>%
  mutate(prop = n/sum(n))
```

We want to sample (1/3, 1/3, 1/3) proportion of observations from each species type.

```{r warning=FALSE}
train <- iris %>%
  mutate(ind = 1:nrow(iris)) %>%
  group_by(Species) %>%
  mutate(n = n()) %>%
  sample_frac(size = 0.5, weight = n) %>%
  ungroup()

# proportion of species types in training set
train %>% 
  dplyr::group_by(Species) %>%
  summarise(n = n()) %>%
  mutate(prop = n/sum(n))

train_ind <- -train$ind

hold_out <- iris[-train_ind, ]
# proportion of species types in hold-out set
hold_out %>% 
  dplyr::group_by(Species) %>%
  summarise(n = n()) %>%
  mutate(prop = n/sum(n))

#knn needs only predictors in train and hold-out matrix
train_cl <- pull(train, Species)
train <- dplyr::select(train, -Species, -ind, -n)
hold_out_cl <- pull(hold_out, Species)
hold_out <- dplyr::select(hold_out, -Species)

```

Next, we use K-nearest neighbor for the prediction of species type. `prop = TRUE` returns predicted probability for the winning species type.  

`knn()` forms predictions using a single command.

We produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified in the hold-out set. 

Comment on the following confusion matrix. [Hint: how many correct predictions on the held-out data and what is the proportion of correct prediction. What is the test set error rate.]


```{r}
library(class)
set.seed(10) # to ensure reproducibility of the results
# Use KNN to predict the labels for hold-out set.
fit_knn <- knn(scale(train), scale(hold_out), cl = train_cl, k = 3, prob = TRUE)
# produce confusion matrix
tab <- table(hold_out_cl, fit_knn)
tab
```


```{r}
mean(hold_out_cl==fit_knn)
diag(tab)[1]
diag(tab)[2]
1-sum(diag(tab))/sum(tab)
```

72 predictions on the held-out data are correct.
96% predictions are correct.
the test set error rate is 4%

\newpage

1.8 Rand index is a measure of agreement between the columns and rows of a 2-way contingency table. Use `classAgreement()` function from _e1071_ package to compute the Rand index for the confusion matrix. Comment on Rand index and Adjusted Rand Index.
```{r}
library(e1071)
?classAgreement()
classAgreement(tab,match.names=FALSE)
```

96% of data points in the main diagonal of tab,94% diag corrected for agreement by chance.
rand index = 0.9491892, rand index corrected for agreement by chance =0.8843673


\newpage

## Question 2

2.1 Consider a classification tree below. It contains classification results for three different types of wine, Type 1, Type 2, and Type 3 (numbered as 1, 2, and 3). The following also explains the summary in the first node. 

In the data set, what percentage of observations are from Type 2?

```{r echo=FALSE, out.width="90%", fig.caption = "Source: Wikipedia", fig.align="center"}
knitr::include_graphics("Q2_ref.png")
```

40% of the data is Type 2.

\newpage
2.2 Based on the above classification tree, what percentage of observations end up being classified by the tree as Type 3?

26% of the observations are been classified as type 3.

\newpage
2.3 Based on the above classification tree, if an observation had a "Proline" value of 620, an "OD280/OD315 of Diluted Wines" value of 2, a "Hue" of 0.8, and a "Flavanoids" value of 2.1, which Type would it be classified as?

it will be classified as type3.

\newpage
## Question 3

3.1 We consider the _bankruptcy_ data from _MixGHD_ package. We will use this data set to predict the status of the firm: bankruptcy (0) or financially sound (1). Read the help page for _bankruptcy_ data from _MixGHD_ package. Identify the predictor variables and response variable. 

```{r }
library(rpart)
library(rattle)
library(MixGHD)
data(bankruptcy,package="MixGHD")


```

predictor variables: RE & EBIT
response variable: the status of the firm.



\newpage
3.2 Find the proportion of observations in the bankruptcy (0) and financially sound (1) classes. 
50% of the observations is in bankruptcy and the other 50% is in financially sound.

\newpage

3.3 We use 60%\/40\% training/hold-out data sets. Run the following code. Show the prediction table and explain it. Make sure to include an interpretation of the analysis and results in terms of the data (first read the background information on the bankruptcy data set from the _MixGHD_ package).

```{r fig.align='center', eval=FALSE}
set.seed(1)

## Use the bankruptcy data frame. Then create an index for each row, "ind". 
# Next group by Y and compute a number of observations for each group. 
# Finally, sample 60% of observations so that the training set has 
# the same proportion of each class of Y as in the original data.
library(dplyr)
bankruptcy_train <- bankruptcy %>%
  mutate(ind = 1:nrow(bankruptcy)) %>%
  group_by(Y) %>%
  mutate(n = n()) %>%
  sample_frac(size = .6, weight = n) %>%
  ungroup()

bankruptcy_train_index <- bankruptcy_train$ind

# Build a tree 
bank_tree <- rpart(Y ~ ., 
                   data = bankruptcy,
                   subset = bankruptcy_train_index,
                   method="class")

# Prints a table of optimal prunings based on a complexity parameter
printcp(bank_tree)
# PLot decision tree
fancyRpartPlot(bank_tree)
# Prediction table
tab <- table(predict(bank_tree, 
              bankruptcy[-bankruptcy_train_index,], 
              type = "class"),
  bankruptcy[-bankruptcy_train_index, "Y"])

```
RE is the only independent variable,With the condition of RE<7.9, half of the observations fail to pass the condition and the other 50% pass. the 50% that pass the condition are predicted as bankruptcy and the other 50% that do not pass the condiction are predicted as financially sound.
root node error is 0.5
\newpage 

3.4 Repeat this analysis without stratified sampling, i.e., use 
```{r}
set.seed(1)
bankruptcy_train_index <- sample(1:nrow(bankruptcy), 
                                 size  = ceiling(.6*nrow(bankruptcy)))
# Build a tree 
bank_tree <- rpart(Y ~ ., 
                   data = bankruptcy,
                   subset = bankruptcy_train_index,
                   method="class")

# Prints a table of optimal prunings based on a complexity parameter
printcp(bank_tree)
# PLot decision tree
fancyRpartPlot(bank_tree)
# Prediction table
tab <- table(predict(bank_tree, 
              bankruptcy[-bankruptcy_train_index,], 
              type = "class"),
  bankruptcy[-bankruptcy_train_index, "Y"])

```

RE is the only independent variable with the condition that RE<-3.6. If the observation passes the condition, it will be predicted as bankruptcy in which case 35% of the observation has been predicted as bankruptcy. The other 65% has been predicted as financiallt sound.
root node error is 0.4.

\newpage

3.5 Compare and contrast the results (of the prediction table) obtained using stratified sampling, to the results obtained using unstratified sampling (a.k.a. random sampling).

Stratified sampling: Prediction of Class'0'and Class '1' are both 100% correct.
unstratified sampling: the Prediction of Class'0' is 100% correct, but for Class '1' has 8% wrong-possibility
In general, stratified sampling predicts better than the unstratidied one.


\newpage
## Reference

\newpage
## Helper's name
Weijiang Song
