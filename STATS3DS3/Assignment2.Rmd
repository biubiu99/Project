---
title: |
  | STATS 3DS3
  | Homework assignment 2
  | Data visualization and KNN classifier 
author: 
  - "Ling Lu"
date: "02/02/2021"
output: 
  pdf_document:
    includes:
      in_header: header.tex
bibliography: STATS3DS3.bib

---

**You may discuss homework problems with other students, but you have to prepare the written assignments yourself.**

**Please combine all your answers, the computer code and the figures into one PDF file, and submit a copy to Crowdmark.**

Please use **newpage** to write solution for each part of a question.

Please choose the pages for each part of a question in Crowdmark. 

**Grading scheme: Except (2.8), $\left\lbrace 0, 1, 2\right\rbrace$ points per question, including reference section. 10 points for (2.8). Total 56 points**. We will convert this to 100\%.

Homework assignment 2 due date: \rc Monday, February 15th, 2021 at 10:00 PM.\bc

\newpage

## Question 1

We will create a word cloud for the STATS 3DS3 outline. The word cloud is made up of all words that appear in the outline more than ten times.

First, we need to download `Syllabus_3DS3_Winter 2021.Rmd` from the "Homework assignment 2" folder on the course website. Then, save it in our working directory.

1.1) Read `Syllabus_3DS3_Winter 2021.Rmd` using `readr::read_line()` and assign it to an object name `rmd_lines`. 

```{r }
getwd()
rmd_lines <- readr::read_lines("syllabus_3DS3_Winter 2021.Rmd")
```

\newpage

1.2) Explore the object `rmd_lines` using `class()`, `head()`, `tail()`. Use appropriate chunk options to hide the results for this question (1.2). Show the R commands in the output.
```{r results='hide'}
class(rmd_lines)
head(rmd_lines)
tail(rmd_lines)
```


\newpage

1.3) Remove any numbers from the object `rmd_lines` using `gsub()`. We can use a `pattern = "[0-9]"` and `replacement = ""`.  

```{r}
rmd_lines1.3 <- gsub(rmd_lines,pattern = "[0-9]",replacement = "")
```


\newpage

1.4) Use the results from (1.3) to make a tibble. Then, use `tidytext::unnest_tokens()` to extract a vector of words.

```{r}

library(tidyverse)
library(tidytext)
rmd_lines1.4 <-as_tibble(rmd_lines1.3) %>% tidytext::unnest_tokens(words,value)
```


\newpage
1.5) Use the results from (1.4). Remove stop words using `anti_join(stop_words)`, where `stop_words` is a data frame with stop words. 

```{r}
rmd_lines1.5 <-rmd_lines1.4 %>% anti_join(stop_words,by=c("words" = "word"))
head(rmd_lines1.5)
```

\newpage
1.6) Use the results from (1.5). Find the top ten most frequently used words in the STATS 3DS3 outline. We can use `count()`, `top_n()`, and `pull()`.

```{r}
rmd_lines1.6 <-rmd_lines1.5 %>% count(vars = words) %>% top_n(10) %>% pull(vars)
print(rmd_lines1.6)
```

\newpage
1.7) Use the results from (1.5). We can see that the top ten most frequently used words have some LaTeX commands. Remove those LaTeX commands, `("bc", "blc", "https")`. You can use `dplyr::filter()`.

```{r}
rmd_lines1.7 <-rmd_lines1.5 %>% filter(words != "bc") %>% filter(words !="blc") %>% filter(words !="https")
head(rmd_lines1.7)
```


\newpage

1.8) Use the results from (1.7). Remove any words with less than ten counts. We can first use `count()` and then `dplyr::filter()`.

```{r}
rmd_lines1.8<-filter(count(rmd_lines1.7,var=words),n>=10)
rmd_lines1.8
```

\newpage
1.9) Use the results from (1.8). Compute the percentage of each word in the outline. We can use `dplyr::mutate()`.

```{r}
rmd_lines1.9 <-mutate(rmd_lines1.8,percentage=n/sum(n)) 
rmd_lines1.9
```


\newpage
1.10) Use the results from (1.9). Produce a word cloud for the STATS 3DS3 outline. We must assign the size of the text to the percentage of words. We can use `geom_text_wordcloud()` function from `ggwordcloud` package.

```{r}
library(ggwordcloud)
ggplot(rmd_lines1.9,aes(label=var, size=percentage))+geom_text_wordcloud()+scale_size_area(max_size = 15)+theme_minimal()
```


\newpage

1.11) Write a summary about the word cloud (less than 120 characters).

Based on the graph from 1.10, we can conclude that the most frequently used word in outline is "week". Other than that "lecture", "students", "academic" and "Homework" are also used many times in the outline compare to other words since their size seems a lot more larger.


\newpage

## Question 2

We will develop a shiny app and provide the website link in the output. We will use [\blc Johns Hopkins Github data on COVID 19\bc](https://github.com/datasets/covid-19) confirmed cases to develop a shiny app. 


2.1) Visit the [\blc website\bc](https://github.com/datasets/covid-19) and read the description. Read the CSV file for [\blc confirmed cases\bc](https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv). We can use `read_csv()`. When we click on \blc confirmed cases\bc in the previous sentence, it will take us to the CSV file.

```{r}
data <- read.csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"))

```

\newpage

2.2) Use the results from (2.1). Rename two variables, "`Province/State`" as "Province" and "`Country/Region`" as "Country". 

```{r}
data2.2 <-rename(data,Province = Province.State,Country=Country.Region)

```

\newpage

2.3) Use the results from (2.2). Select the confirmed case data for Canada. We can use  `dplyr::filter()`.

```{r}
data2.3 <- data2.2 %>% filter(Country == "Canada")
```


\newpage

2.4) Use the results from (2.3). Remove confirmed cases for `Diamond Princess`, `Grand Princess`, and `Repatriated Travellers`. Now we must have time series data of confirmed cases for ten provinces and three territories in Canada.
```{R}
data2.4 <-data2.3%>% filter(Province !="Diamond Princess") %>% filter(Province !="Grand Princess") %>% filter(Province !="Repatriated Travellers")

```

\newpage

2.5) Use the results from (2.4). We can gather all dates and make one column for date using `tidyr::gather(data_frame, key = "date", value = "confirmed_cases", -var1, -var2, -var3, -var4)`.  
```{r}
data2.5 <- data2.4 %>% gather(key = "date", value = "confirmed_cases", -Province, -Country, -Lat, -Long)

```

\newpage

2.6) Use the results from (2.5). Convert the character variable date to a date format. We can use the "%m/%d/%y" date format in `as.Date()`. More information on date formats can be found [\blc here.\bc](https://www.statmethods.net/input/dates.html)

```{r}
data2.5$date <- as.Date(data2.5$date,"X%m.%d.%y")
head(data2.5)
```


\newpage
2.7) Use the results from (2.6). Create time series plots for confirmed cases in each province and territory in one plot. Color the lines by provinces and territories. 

```{r}
ggplot(data2.5,aes(date,confirmed_cases))+ geom_line(aes(color = Province))
```

\newpage


2.8) Let us create a shiny app. User input is a starting date. Show the time series plot for confirmed cases from this date. Include time series plot for confirmed cases in each province and territory in one plot. Color the lines by provinces and territories. The user interface for the shiny app and some R commands to write a server function are provided. Publish your shiny app at shinyapp.io. 

For this question, we can create an app separately. Only show the R commands and the website link to your shiny app in the PDF output.


```{r eval=FALSE}
library(shiny)
ui <- fluidPage(
  
  dateInput('starting_date',
      label = 'Starting date in yyyy-mm-dd format',
      value = as.character("2021-02-01"),
      min = "2020-01-22", max = Sys.Date(),
      format = "yyyy-mm-dd",
      startview = 'year', language = 'en', weekstart = 1
    ),
  
  plotOutput(outputId = "time_series_plot" 
    
  ) 
)

server <- function(input, output){
  
  output$time_series_plot <- renderPlot({
    
    # You can use the R commands written from (2.1) to (2.6)
data <- read.csv(url("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv"))
data
data2.2 <-rename(data,Province = Province.State,Country=Country.Region)
data2.3 <- data2.2 %>% filter(Country == "Canada") 
data2.4 <-data2.3%>% filter(Province !="Diamond Princess") %>% filter(Province !="Grand Princess") %>% filter(Province !="Repatriated Travellers")
data2.5 <- data2.4 %>% gather(key = "date", value = "confirmed_cases", -Province, -Country, -Lat, -Long)  
data2.5$date <- as.Date(data2.5$date,"X%m.%d.%y")
confirmed_Canada <-data2.5
    
    
    # Let's say the data frame (tibble) 
    # you have by doing (2.1) - (2.6) is 
    # "confirmed_Canada". 
    # Now you can use the following to 
    # use the input date to select the rows: 
    confirmed_Canada <- filter(confirmed_Canada, date >= as.Date(as.character(input$starting_date)))
    
    # Use the R commands written for (2.7) to create the plot from confirmed_Canada data frame
    
ggplot(confirmed_Canada,aes(date,confirmed_cases))+ geom_line(aes(color = Province))    
    
  }
                                        )
}

# Run the app
shinyApp(ui = ui, server = server)
```

the link to the website: https://biubiu2333.shinyapps.io/app288/


\newpage

## Question 3

We will consider an exercise from **ISLR** Exercise 4.7. We can access the first edition of the book: [\blc click here.\bc](https://static1.squarespace.com/static/5ff2adbe3fe4fe33db902812/t/6009dd9fa7bc363aa822d2c7/1611259312432/ISLR+Seventh+Printing.pdf)

I reproduce Exercise 4.7 here:

When the number of features $p$ is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when $p$ is large. We will now investigate this curse.

3.1) Suppose that we have a set of observations, each with measurements on $p = 1$ feature, $X$. We assume that $X$ is uniformly (evenly) distributed on $[0, 1]$. Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10\% of the range of $X$ closest to that test observation. For instance, in order to predict the response for a test observation with $X = 0.6$, we will use observations in the range $[0.55, 0.65]$. On average, what fraction of the available observations will we use to make the prediction?

[Hint: we can use simulation.]

```{r}
library(tidyverse)
library(plotly)
n=10000
x1 <-tibble(x1=runif(n))
mean(x1$x1 <=0.65 & x1$x1>=0.55)
```

on average, about 10% of the available observations will be used to make the prediction.

\newpage

3.2) Now suppose that we have a set of observations, each with measurements on $p = 2$ features, $X_{1}$ and $X_{2}$. We assume that $(X_{1} , X_{2})$ are uniformly distributed on $[0, 1] \times [0, 1]$. We wish to predict a test observation's response using only observations that are within 10\% of the range of X_{1} and within 10\% of the range of $X_{2}$ closest to that test observation. For instance, in order to predict the response for a test observation with $X_{1} = 0.6$ and $X_{2} = 0.35$, we will use observations in the range $[0.55, 0.65]$ for $X_{1}$ and in the range $[0.3, 0.4]$ for $X_{2}$. On average, what fraction of the available observations will we use to make the prediction?

[Hint: we can use simulation.]

```{r}
x1x2 <-tibble(x1=runif(n),x2=runif(n))
mean(x1x2$x1<=0.65 & x1x2$x1 >=0.55 & x1x2$x2 >=0.3 &x1x2$x2 <=0.4)

```

on average, about 1% of the available observations will be used to make the prediction.

\newpage

3.3) Now suppose that we have a set of observations on $p = 100$ features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10\% of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?

[Hint: we can guess this based on the results for (3.1) and (3.2).]

based on the result from 3.1 and 3.2, about 0.1^100 of the observations will be used to make the prediction.
\newpage

3.4) Using your answers to parts (3.1)–(3.3), argue that a drawback of KNN when $p$ is large is that there are very few training observations ``near" any given test observation.

a fraction of available observations near of the test observation decreases exponentially by number of predictors.
\newpage

## Reference
get help from Lan Wang & Weijiang song

